# -*- coding: utf-8 -*-
"""Data Cleaning in Google Colab.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/167hEw52b1ll9WLWOjHJU7YgA9acVKvCi
"""

# Data was downloaded from the NCBI website.
# For more explanation on the dataset, please refer to the google document.
# Each of the sequence files below has 200 or more than 200 sequences.
# Similar steps are performed in the AWS Sagemaker Jupyter Notebook but on 80+GB of data
# This was first performed in the colab evnironment for practices purposes.

# Upload files from the PC into google colab evnironment.
import pandas as pd
import seaborn as sns
import geopandas as gpd
from google.colab import files
import matplotlib.pyplot as plt


# Upload the metadata CSV file, .acc file, and FASTA files
uploaded = files.upload()

# Read the metadata CSV
# Find the metadata CSV file among the uploaded files
# Load its content into a pandas DataFrame for analysis.
metadata_file = [filename for filename in uploaded.keys() if filename.endswith('.csv')][0]
metadata_df = pd.read_csv(metadata_file)

# Rename the accession column for consistency across all datasets which need to be merged
metadata_df.rename(columns={'Accession': 'accession_id'}, inplace=True)

# Print the column names and the first few rows of the metadata
print("\nMetadata CSV columns and first few rows:")
# Print column names
print(metadata_df.columns)
# Print the first few rows to inspect data
# print(metadata_df.head())

# Read the .acc file (which contains accession IDs)
# Split it into a list of accession IDs
acc_file = [filename for filename in uploaded.keys() if filename.endswith('.acc')][0]
with open(acc_file, 'r') as f:
    acc_list = f.read().splitlines()

# And convert acc_list to a DataFrame for merging with the other two datasets
acc_df = pd.DataFrame(acc_list, columns=['accession_id'])

# Upload and read the FASTA files
fasta_files = [filename for filename in uploaded.keys() if filename.endswith('.fasta')]
fasta_data = {}

# Read FASTA files
# Store the sequences by accession ID
for fasta_file in fasta_files:
    with open(fasta_file, 'r') as file:
        fasta_content = file.read()
        # Extract accession ID from the first line of the FASTA file
        # Extract only the first part
        # Store the sequences in a dictionary with the accession IDs as keys.
        accession_id = fasta_content.split("\n")[0].split()[0].replace(">", "")
        fasta_data[accession_id] = fasta_content

# Convert fasta_data dictionary to a DataFrame
fasta_df = pd.DataFrame(list(fasta_data.items()), columns=['accession_id', 'fasta_sequence'])

# Print column names of all datasets
print("\nColumns in metadata DataFrame:", metadata_df.columns)
print("Columns in accession DataFrame:", acc_df.columns)
print("Columns in FASTA DataFrame:", fasta_df.columns)

# Merge all datasets on 'accession_id'
merged_df = acc_df.merge(metadata_df, on='accession_id', how='left').merge(fasta_df, on='accession_id', how='left')

# Verify if the data was merged successfully
# Display first few rows
print(merged_df.head())

# Check the shape (rows, columns)
print("\nShape of merged dataset:", merged_df.shape)

# Check the number of sequences in the dataset
print("\nNumber of unique sequences:", merged_df['fasta_sequence'].nunique())

# Check the number of unique accession ids in the merged dataset
print("\nUnique accession IDs:", merged_df['accession_id'].nunique())

# Check for missingness in all columns
print("\nMissing values per column:\n", merged_df.isnull().sum())

# Check if there are duplicate accession ids
print("\nDuplicate accession IDs:", merged_df['accession_id'].duplicated().sum())

# Check datatypes of all columns in the merged dataset
print("\nColumn data types:\n", merged_df.dtypes)

# Convert date columns to datetime formats
merged_df['Collection_Date'] = pd.to_datetime(merged_df['Collection_Date'], errors='coerce')

# Check sequence length
merged_df['sequence_length'] = merged_df['fasta_sequence'].str.len()
print("\nSummary of sequence lengths:\n", merged_df['sequence_length'].describe())

# Identify and remove any outliers in sequence length based on Q1, Q3 and Inter Quartile Range
Q1 = merged_df['sequence_length'].quantile(0.25)
Q3 = merged_df['sequence_length'].quantile(0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

outliers = merged_df[(merged_df['sequence_length'] < lower_bound) | (merged_df['sequence_length'] > upper_bound)]
print("\nNumber of outlier sequences:", outliers.shape[0])

# Remove outliers if necessary
# merged_df = merged_df[(merged_df['sequence_length'] >= lower_bound) & (merged_df['sequence_length'] <= upper_bound)]

# Check unique organisms -- All of these should be SARS-CoV-2
# Cause only these with their relevant ID were downloaded from the NCBI website
print("\nUnique organism names:", merged_df['Organism_Name'].unique())

# Check for genome completeness -- All of these should be complete
print("\nGenome completeness values:", merged_df['Nuc_Completeness'].value_counts())

# Check for missing collection dates -- Ideally none of these should be missing
missing_dates = merged_df['Collection_Date'].isnull().sum()
print("\nMissing collection dates:", missing_dates)

# Extract year from collection date
merged_df['Year'] = merged_df['Collection_Date'].dt.year
print("\nYear distribution:\n", merged_df['Year'].value_counts())

# Check for what is the distribution of countries
print("\nTop 10 countries in dataset:\n", merged_df['Country'].value_counts().head(10))

# Country names -- remove any leading or trailing spaces
# Capitalize the first letter of each word using str.title().
merged_df['Country'] = merged_df['Country'].str.strip().str.title()

# Check pango lineage distribution
print("\nTop 10 Pango lineages:\n", merged_df['Pangolin'].value_counts().head(10))

# Remove unnecessary columns
columns_to_drop = ['GenBank_RefSeq', 'Submitters', 'BioSample', 'BioProject', 'Publications']
merged_df.drop(columns=columns_to_drop, inplace=True)

# Check for missing pangolin Lineages
missing_pango = merged_df['Pangolin'].isnull().sum()
print("\nMissing Pango lineage values:", missing_pango)

# Encode categorical variables
merged_df['Country'] = merged_df['Country'].astype('category').cat.codes
merged_df['Pangolin'] = merged_df['Pangolin'].astype('category').cat.codes

# Check feature correlations
# Select only numeric columns from the data
numeric_df = merged_df.select_dtypes(include=['number'])

# Calculate the correlation matrix for numeric columns only
corr_matrix = numeric_df.corr()

# Plot the heatmap
plt.figure(figsize=(10, 6))
sns.heatmap(corr_matrix, annot=True, cmap="coolwarm", fmt=".2f")
plt.show()

# Check feature distributions
merged_df.hist(figsize=(12, 8), bins=30)
plt.show()

# Convert to lower case for consistency
merged_df.columns = merged_df.columns.str.lower()

# Check memory usage if need be
# print("\nMemory usage before optimization:")
# print(merged_df.memory_usage(deep=True))

# Save cleaned data if need be
# merged_df.to_csv("cleaned_dataset.csv", index=False)

# Download the CSV file to PC if required
# files.download('cleaned_dataset.csv')

# Check column names in the DataFrame
print(merged_df.columns)

# Collection_date should be in the datetime format
merged_df['collection_date'] = pd.to_datetime(merged_df['collection_date'], errors='coerce')

# Extract the Year from the collection_date
merged_df['Year'] = merged_df['collection_date'].dt.year

# Plot the boxplot
plt.figure(figsize=(10, 5))
sns.boxplot(x=merged_df['Year'], y=merged_df['sequence_length'])
plt.xticks(rotation=45)
plt.title("Genome Sequence Length Distribution by Year")
plt.xlabel("Year")
plt.ylabel("Sequence Length")
plt.show()

# Distribution of sequence collected by month
merged_df['Month'] = merged_df['collection_date'].dt.month

plt.figure(figsize=(10, 5))
sns.countplot(x=merged_df['Month'], palette='coolwarm')
plt.xticks(range(0, 12), ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'])
plt.title("Distribution of Collected Sequences by Month")
plt.xlabel("Month")
plt.ylabel("Number of Sequences")
plt.show()

# Most frequently found SARS-CoV-2 lineages
# Top 15 most common lineages
top_pango = merged_df['pangolin'].value_counts().head(15)

plt.figure(figsize=(12, 5))
sns.barplot(x=top_pango.index, y=top_pango.values, palette="magma")
plt.xticks(rotation=45)
plt.title("Top 15 SARS-CoV-2 Lineages in Dataset")
plt.xlabel("Pango Lineage")
plt.ylabel("Count")
plt.show()

# Most frequently found host species
top_hosts = merged_df['host'].value_counts().head(10)

plt.figure(figsize=(10, 5))
sns.barplot(x=top_hosts.index, y=top_hosts.values, palette="viridis")
plt.xticks(rotation=45)
plt.title("Most Common Host Species for SARS-CoV-2")
plt.xlabel("Host Species")
plt.ylabel("Count")
plt.show()

# Geographic distribution of sequences
# Map
# First we will need to load the world map data from Natural Earth
world = gpd.read_file("https://naturalearth.s3.amazonaws.com/110m_cultural/ne_110m_admin_0_countries.zip")

# Count sequences per country
country_counts = merged_df['country'].value_counts().reset_index()
country_counts.columns = ['country', 'Count']

# Make sure both columns are strings before merging
world['NAME'] = world['NAME'].astype(str)
country_counts['country'] = country_counts['country'].astype(str)

# Merge world map with sequence data
world = world.merge(country_counts, left_on="NAME", right_on="country", how="left")

# Plot
fig, ax = plt.subplots(figsize=(12, 6))
world.boundary.plot(ax=ax, linewidth=1, color="black")  # Plot country boundaries
world.plot(column='Count', cmap='OrRd', linewidth=0.5, edgecolor='black', legend=True, ax=ax)

plt.title("Geographic Distribution of SARS-CoV-2 Sequences")
plt.show()

# Relationship between sequence length and pango lineage
plt.figure(figsize=(12, 6))
sns.boxplot(x=merged_df['pangolin'], y=merged_df['sequence_length'])
plt.xticks(rotation=90)
plt.title("Genome Lengths Across Pango Lineages")
plt.xlabel("Pango Lineage")
plt.ylabel("Sequence Length")
plt.show()

# Lineage evolution over time
lineage_over_time = merged_df.groupby(['year', 'pangolin']).size().reset_index(name='count')

plt.figure(figsize=(12, 6))
sns.lineplot(data=lineage_over_time, x='year', y='count', hue='pangolin', marker='o', legend=None)
plt.title("SARS-CoV-2 Lineages Over Time")
plt.xlabel("Year")
plt.ylabel("Number of Sequences")
plt.show()

# Sequence completeness vs. length
plt.figure(figsize=(8, 5))
sns.boxplot(x=merged_df['nuc_completeness'], y=merged_df['sequence_length'])
plt.title("Genome Lengths by Completeness")
plt.xlabel("Nucleotide Completeness")
plt.ylabel("Sequence Length")
plt.show()

# Frequency of sampling by country over time
plt.figure(figsize=(12, 6))
sns.histplot(data=merged_df, x='year', hue='country', multiple="stack", palette="tab10")
plt.title("Sequence Contributions by Country Over Time")
plt.xlabel("Year")
plt.ylabel("Number of Sequences")
plt.show()

# Sequence length distribution by country
plt.figure(figsize=(12, 6))
sns.boxplot(x=merged_df['country'], y=merged_df['sequence_length'])
plt.xticks(rotation=90)
plt.title("Genome Lengths Across Countries")
plt.xlabel("Country")
plt.ylabel("Sequence Length")
plt.show()